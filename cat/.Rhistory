set.seed(12345) # for repeatability of random numbers
datafileName<-"c:\\idm\\Step4ReducedNatGeoFileAP1.xlsx" # source file
################################################################
##### 1 - READ DATA FILE INTO R AND PERFORM DATA PREPROCESSING
################################################################
df<-read.xls(datafileName,sheet=1,header=TRUE)
numSuppliers<-length(unique(df$Supplier.ID))
## Create blank dataframe
supplier<-sort(unique(df$Supplier.ID),decreasing=FALSE) # supplierIDs in asc order
description<-c(rep("",numSuppliers))
procdf<-data.frame(supplier,description)
procdf$description<-as.character(procdf$description)
## populate dataframe with text for each supplier
for (ctr in 1:numSuppliers){
# get a list of rows from original df that has matching suppliers
idx<-which(df$Supplier.ID==supplier[ctr])
# figure out index of rows in original file with this supplier
l<-length(idx)
workingText<-c("")
# for each supplier, concatenate text of interest into working text
for (j in 1:l){
rowidx<-idx[j]
workingText<-paste0(workingText,df[rowidx,3])
}
procdf[ctr,2]<-
vapply(lapply(strsplit(workingText, " "), unique),
paste, character(1L), collapse = " ")
}
################################################################
##### 2 - CREATE CORPUS
################################################################
corpus<-Corpus(VectorSource(procdf$description))
cleancorpus<-tm_map(corpus,removeWords,stopwords("english"))
#cleancorpus<-tm_map(cleancorpus,stripWhitespace)
dtm<-DocumentTermMatrix(cleancorpus)
dtm_tfxidf<-weightTfIdf(dtm)
m<-as.matrix(dtm_tfxidf)
rownames(m) <- 1:nrow(m)
norm_eucl <- function(m) m/apply(m, MARGIN=1, FUN=function(x) sum(x^2)^.5)
m_norm <- norm_eucl(m)
# we have four rows that are full of zeroes
cl <- kmeans(m_norm, 50)
test<-rowSums(dtm_tfxidf)
test<-sum(dtm_tfxidf)
test<-rowSums(m)
which(test==0)
ncol(m)
zerov<-rep(0,ncol(m))
erridx<-which(test==0)
test<-rowSums(m)
erridx<-which(test==0)
zerov<-rep(0,ncol(m))
for(x in 1:len(erridx)){
m[x,]<-zerov
}
for(x in 1:length(erridx)){
m[x,]<-zerov
}
loc<-erridx[x]
m<-as.matrix(dtm_tfxidf)
rownames(m) <- 1:nrow(m)
test<-rowSums(m)
erridx<-which(test==0)
zerov<-rep(0,ncol(m))
for(x in 1:length(erridx)){
loc<-erridx[x]
m[loc,]<-zerov
}
norm_eucl <- function(m) m/apply(m, MARGIN=1, FUN=function(x) sum(x^2)^.5)
m_norm <- norm_eucl(m)
# we have four rows that are full of zeroes
cl <- kmeans(m_norm, 50)
View(procdf)
?tm_map
getTransformations()
#######################################
##### O - INITIALIZE SCRIPT PARAMETERS
#######################################
## Load the needed libraries and functions
library(gdata)
library(tm)
## Set script parameters
set.seed(12345) # for repeatability of random numbers
datafileName<-"c:\\idm\\Step4ReducedNatGeoFileAP1.xlsx" # source file
################################################################
##### 1 - READ DATA FILE INTO R AND PERFORM DATA PREPROCESSING
################################################################
df<-read.xls(datafileName,sheet=1,header=TRUE)
numSuppliers<-length(unique(df$Supplier.ID))
## Create blank dataframe
supplier<-sort(unique(df$Supplier.ID),decreasing=FALSE) # supplierIDs in asc order
description<-c(rep("",numSuppliers))
procdf<-data.frame(supplier,description)
procdf$description<-as.character(procdf$description)
for (ctr in 1:numSuppliers){
# get a list of rows from original df that has matching suppliers
idx<-which(df$Supplier.ID==supplier[ctr])
# figure out index of rows in original file with this supplier
l<-length(idx)
workingText<-c("")
# for each supplier, concatenate text of interest into working text
for (j in 1:l){
rowidx<-idx[j]
workingText<-paste0(workingText,df[rowidx,3])
}
procdf[ctr,2]<-
vapply(lapply(strsplit(workingText, " "), unique),
paste, character(1L), collapse = " ")
}
corpus<-Corpus(VectorSource(procdf$description))
cleancorpus<-tm_map(corpus,removeWords,stopwords("english"))
cleancorpus<-tm_map(cleancorpus,stemDocument)
dtm<-DocumentTermMatrix(cleancorpus)
dtm_tfxidf<-weightTfIdf(dtm)
m<-as.matrix(dtm_tfxidf)
rownames(m) <- 1:nrow(m)
?TermDOcumentMatrix
install.packages("textreuse")
library(textreuse)
?lsh
?system.file
?TextReuseCorpus
corpus
?corpus
?`textreuse-package`
vignette("textreuse-introduction", package = "textreuse")
library(textreuse)
library(gdata)
fileLocation<-"C:\\Users\\koushikk\\repos\\XeevaIDM\\LSH\\ActiveSKUDescInput.xlsx"
rawdf = read.xls (fileLocation, sheet = 1, header = TRUE)
?tokenize_ngrams
a<-"S874934;GRANIT2D INDUSTIRAL ER FOCUS"
b<-"XO RUST GLOSS BLACK SPRAY PAINT"
hash_string(a)
hash_string(b)
?read.csv
library(textreuse)
library(gdata)
fileLocation<-"C:\\Users\\koushikk\\repos\\XeevaIDM\\LSH\\ActiveSKUDescInput.csv"
###################################################################################
######## READING AND PRE-PROCESSING THE INPUT FILE
###################################################################################
rawdf = read.csv(fileLocation)
View(rawdf)
library(textreuse)
library(gdata)
fileLocation<-"C:\\Users\\koushikk\\repos\\XeevaIDM\\LSH\\ActiveSKUDescInput.csv"
###################################################################################
######## READING AND PRE-PROCESSING THE INPUT FILE
###################################################################################
rawdf = read.csv(fileLocation)
rawdf$Hash<-hash_string(rawdf$Product.Name)
numRec<-nrows(rawdf)
?nrows
?nrow
numRec<-nrow(rawdf)
hasvalues<-numeric(numRec)
hashValue<-numeric(numRec)
for (ctr in 1:numRec){
hashValue[ctr]<-hash_string(rawdf$Product.Name)
}
?read.csv
rawdf = read.csv(fileLocation,stringsAsFactors=FALSE)
rawdf$Hash<-hash_string(rawdf$Product.Name)
View(rawdf)
?lsh
test<-TextReuseTextDocument(rawdf$Product.Name)
test<-TextReuseCorpus(rawdf$Product.Name,meta = list(rawdf$Product.Sku))
test<-TextReuseTextDocument(rawdf[1,2],meta = list(rawdf[1,1))
test<-TextReuseTextDocument(rawdf[1,2],meta = list(rawdf[1,1))
test<-TextReuseTextDocument(rawdf[1,2],meta = list(rawdf[1,1])
)
test<-TextReuseTextDocument(rawdf[1,2],meta = list("id" = rawdf[1,1])
)
test
corpus<-TextReuseCorpus(text=rawdf$Product.Name,meta= list("id"= rawdf$Product.Sku))
warnings()
?lsh
result<-lsh(corpus,40,progress = interactive())
test<-TextReuseCorpus(rawdf$Product.Name,meta = list("id" = rawdf$Product.Sku))
?TextReuseCorpus
corpus<-TextReuseCorpus(text=rawdf$Product.Name,
meta= list("id"= rawdf$Product.Sku),
tokenizer = tokenize_ngrams, n = 5)
corpus<-TextReuseCorpus(text=rawdf$Product.Name,
meta= list("id"= rawdf$Product.Sku),
tokenizer = tokenize_ngrams, n = 2)
warnings()
skipped()
skipped(corpus)
?wordcount
rawdf$descWordCount<-wordcount(rawdf$Product.Name)
rawdf = read.csv(fileLocation,stringsAsFactors=FALSE)
numRec <- nrow(rawdf)
rawdf$Word.Count<-NULL
for (ctr in 1:numRec){
rawdf$Word.Count[ctr]<-wordcount(rawdf$Product.Name[ctr])
}
View(rawdf)
rmidx<-which(rawdf$Word.Count<minWords)
minWords<-3
rmidx<-which(rawdf$Word.Count<minWords)
df<-rawdf[-rmidx]
df<-rawdf[-rmidx,]
corpus<-TextReuseCorpus(text=rawdf$Product.Name,
meta= list("id"= rawdf$Product.Sku),
tokenizer = tokenize_ngrams, n = 2)
corpus<-TextReuseCorpus(text=df$Product.Name,
meta= list("id"= df$Product.Sku),
tokenizer = tokenize_ngrams, n = 2)
result<-lsh(corpus,40,progress = interactive())
?minhashes
hashlist<-hashes(corpus)
minhaslist<-minhashes(corpus)
corpus<-TextReuseCorpus(text=df$Product.Name,
meta= list("id"= df$Product.Sku),
tokenizer = tokenize_ngrams, n = 2,
minhash_func = minhash_generator())
minhaslist<-minhashes(corpus)
result<-lsh(corpus,40,progress = interactive())
keyClusters<-unique(rawdf$KEY)
numClusters<-length(keyClusters)
?seq
clusterID<-seq(1,numClusters)
clusterMaster<-merge(clusterID,keyClusters)
rm(clusterMaster)
clusterMaster<-cbind.data.frame(clusterID,keyClusters)
?cbind.data.frame
rm(clusterMaster)
clusterMaster<-cbind.data.frame(clusterID,keyClusters,stringsAsFactors=FALSE)
test<-merge(rawdf,keyClusters,by.x="KEY",by.y = "keyClusters")
?lookup
library(qdap)
install.packages("qdap")
library(qdap)
head(match(rawdf$KEY,clusterMaster$keyClusters))
clusterMaster(head(match(rawdf$KEY,clusterMaster$keyClusters)),)
clusterMaster[head(match(rawdf$KEY,clusterMaster$keyClusters)),]
clusterMaster[head(match(rawdf$KEY,clusterMaster$keyClusters)),1]
rawdf$CID<-clusterMaster[match(rawdf$KEY,clusterMaster$keyClusters),1]
View(rawdf)
library(textreuse)
fileLocation<-"C:\\Users\\koushikk\\repos\\XeevaIDM\\LSH\\InputUniqueSKUMPN.csv"
SKUFID_CTR<-1000000000
###################################################################################
######## PRE-REQUISITE
###################################################################################
## PP1 Key field is created in Catalog file by joining MPN + Price
## PP2 All MPN + Price Keys that have > 1 records associated as well as
##    those MPNs with lengths >= MPN_LEN_THRESHOLD are considered for analysis
## PP3 The records where Product_SKU is not unique are also discarded from analysis
## PP4 Commas are replaced by empty space in above to get input file
###################################################################################
######## STEP 0. READING AND PRE-PROCESSING THE INPUT FILE
###################################################################################
## Read the file into a dataframe
rawdf = read.csv(fileLocation,stringsAsFactors=FALSE)
numRec <- nrow(rawdf) # number of rows
## Remove unwanted fields from the dataframe
rawdf$Active.Inactive.Status=NULL
rawdf$Createddate=NULL
rawdf$Created.Before.2016=NULL
rawdf$Max..Contract.Fromdate=NULL
rawdf$Max..Contract.Todate=NULL
## Initialize new fields for use
rawdf$Word.Count<-NULL
rawdf$Hash.Value<-NULL
rawdf$SKUFID<-0
library(textreuse)
fileLocation<-
"C:\\Users\\koushikk\\repos\\XeevaIDMPrototypes\\LSH\\InputUniqueSKUMPN.csv"
SKUFID_CTR<-1000000000
###################################################################################
######## PRE-REQUISITE
###################################################################################
## PP1 Key field is created in Catalog file by joining MPN + Price
## PP2 All MPN + Price Keys that have > 1 records associated as well as
##    those MPNs with lengths >= MPN_LEN_THRESHOLD are considered for analysis
## PP3 The records where Product_SKU is not unique are also discarded from analysis
## PP4 Commas are replaced by empty space in above to get input file
###################################################################################
######## STEP 0. READING AND PRE-PROCESSING THE INPUT FILE
###################################################################################
## Read the file into a dataframe
rawdf = read.csv(fileLocation,stringsAsFactors=FALSE)
numRec <- nrow(rawdf) # number of rows
## Remove unwanted fields from the dataframe
rawdf$Active.Inactive.Status=NULL
rawdf$Createddate=NULL
rawdf$Created.Before.2016=NULL
rawdf$Max..Contract.Fromdate=NULL
rawdf$Max..Contract.Todate=NULL
## Initialize new fields for use
rawdf$Word.Count<-NULL
rawdf$Hash.Value<-NULL
rawdf$SKUFID<-0
## Simplify Keys to cluster IDs
keyClusters<-unique(rawdf$KEY)
numClusters<-length(keyClusters)
clusterID<-seq(1,numClusters)
clusterMaster<-cbind.data.frame(clusterID,keyClusters,stringsAsFactors=FALSE)
library(textreuse)
fileLocation<-
"C:\\Users\\koushikk\\repos\\XeevaIDMPrototypes\\LSH\\InputUniqueSKUMPN.csv"
SKUFID_CTR<-1000000000
###################################################################################
######## PRE-REQUISITE
###################################################################################
## PP1 Key field is created in Catalog file by joining MPN + Price
## PP2 All MPN + Price Keys that have > 1 records associated as well as
##    those MPNs with lengths >= MPN_LEN_THRESHOLD are considered for analysis
## PP3 The records where Product_SKU is not unique are also discarded from analysis
## PP4 Commas are replaced by empty space in above to get input file
###################################################################################
######## STEP 0. READING AND PRE-PROCESSING THE INPUT FILE
###################################################################################
## Read the file into a dataframe
rawdf = read.csv(fileLocation,stringsAsFactors=FALSE)
numRec <- nrow(rawdf) # number of rows
## Remove unwanted fields from the dataframe
rawdf$Active.Inactive.Status=NULL
rawdf$Createddate=NULL
rawdf$Created.Before.2016=NULL
rawdf$Max..Contract.Fromdate=NULL
rawdf$Max..Contract.Todate=NULL
## Initialize new fields for use
rawdf$Word.Count<-NULL
rawdf$Hash.Value<-NULL
rawdf$SKUFID<-0
## Simplify Keys to cluster IDs
keyClusters<-unique(rawdf$MPN_PRICE)
numClusters<-length(keyClusters)
clusterID<-seq(1,numClusters)
clusterMaster<-cbind.data.frame(clusterID,keyClusters,stringsAsFactors=FALSE)
## Add cluster IDs back to rawdf
rawdf$CID<-clusterMaster[match(rawdf$MPN_PRICE,clusterMaster$keyClusters),1]
## Add cluster size to ClusterMaster
clusterMaster$size<-0
for(ctr in 1:numClusters){
clusterMaster$size[ctr]<-sum(
which(rawdf$CID==clusterMaster$clusterID[ctr]))
}
View(clusterMaster)
sum(clusterMaster$size)
library(textreuse)
fileLocation<-
"C:\\Users\\koushikk\\repos\\XeevaIDMPrototypes\\LSH\\InputUniqueSKUMPN.csv"
SKUFID_CTR<-1000000000
###################################################################################
######## PRE-REQUISITE
###################################################################################
## PP1 Key field is created in Catalog file by joining MPN + Price
## PP2 All MPN + Price Keys that have > 1 records associated as well as
##    those MPNs with lengths >= MPN_LEN_THRESHOLD are considered for analysis
## PP3 The records where Product_SKU is not unique are also discarded from analysis
## PP4 Commas are replaced by empty space in above to get input file
###################################################################################
######## STEP 0. READING AND PRE-PROCESSING THE INPUT FILE
###################################################################################
## Read the file into a dataframe
rawdf = read.csv(fileLocation,stringsAsFactors=FALSE)
numRec <- nrow(rawdf) # number of rows
## Remove unwanted fields from the dataframe
rawdf$Active.Inactive.Status=NULL
rawdf$Createddate=NULL
rawdf$Created.Before.2016=NULL
rawdf$Max..Contract.Fromdate=NULL
rawdf$Max..Contract.Todate=NULL
## Initialize new fields for use
rawdf$Word.Count<-NULL
rawdf$Hash.Value<-NULL
rawdf$SKUFID<-0
## Simplify Keys to cluster IDs
keyClusters<-unique(rawdf$MPN_PRICE)
numClusters<-length(keyClusters)
clusterID<-seq(1,numClusters)
clusterMaster<-cbind.data.frame(clusterID,keyClusters,stringsAsFactors=FALSE)
## Add cluster IDs back to rawdf
rawdf$CID<-clusterMaster[match(rawdf$MPN_PRICE,clusterMaster$keyClusters),1]
## Add cluster size to ClusterMaster
clusterMaster$size<-0
for(ctr in 1:numClusters){
clusterMaster$size[ctr]<-length(
which(rawdf$CID==clusterMaster$clusterID[ctr]))
}
View(rawdf)
View(clusterMaster)
sum(clusterMaster$size)
clusterMaster$tagged<-0
for(ctr in 1:numClusters){
clusterMaster$tagged[ctr]<-length(
which(rawdf$CID==clusterMaster$clusterID[ctr]&&
rawdf$SKUFID!=0))
}
View(clusterMaster)
which(clusterMaster$tagged>0)
clusterMaster$tagged<-0
which(rawdf$CID==clusterMaster$clusterID[1]&&
rawdf$SKUFID!=0)
which(rawdf$CID==clusterMaster$clusterID[2]&&
rawdf$SKUFID!=0)
View(clusterMaster)
View(rawdf)
library(textreuse)
fileLocation<-
"C:\\Users\\koushikk\\repos\\XeevaIDMPrototypes\\LSH\\InputUniqueSKUMPN.csv"
SKUFID_CTR<-1000000000
###################################################################################
######## PRE-REQUISITE
###################################################################################
## PP1 Key field is created in Catalog file by joining MPN + Price
## PP2 All MPN + Price Keys that have > 1 records associated as well as
##    those MPNs with lengths >= MPN_LEN_THRESHOLD are considered for analysis
## PP3 The records where Product_SKU is not unique are also discarded from analysis
## PP4 Commas are replaced by empty space in above to get input file
###################################################################################
######## STEP 0. READING AND PRE-PROCESSING THE INPUT FILE
###################################################################################
## Read the file into a dataframe
rawdf = read.csv(fileLocation,stringsAsFactors=FALSE)
numRec <- nrow(rawdf) # number of rows
## Remove unwanted fields from the dataframe
rawdf$Active.Inactive.Status=NULL
rawdf$Createddate=NULL
rawdf$Created.Before.2016=NULL
rawdf$Max..Contract.Fromdate=NULL
rawdf$Max..Contract.Todate=NULL
## Initialize new fields for use
rawdf$Word.Count<-NULL
rawdf$Hash.Value<-NULL
rawdf$SKUFID<-0
## Simplify Keys to cluster IDs
keyClusters<-unique(rawdf$MPN_PRICE)
numClusters<-length(keyClusters)
clusterID<-seq(1,numClusters)
clusterMaster<-cbind.data.frame(clusterID,keyClusters,stringsAsFactors=FALSE)
## Add cluster IDs back to rawdf
rawdf$CID<-clusterMaster[match(rawdf$MPN_PRICE,clusterMaster$keyClusters),1]
## Add cluster size to ClusterMaster
clusterMaster$size<-0
for(ctr in 1:numClusters){
clusterMaster$size[ctr]<-length(
which(rawdf$CID==clusterMaster$clusterID[ctr]))
}
## Add word count and Hash Value parameters to the dataframe
for (ctr in 1:numRec){
rawdf$Word.Count[ctr]<-wordcount(rawdf$Product.Name[ctr])
rawdf$Hash.Value[ctr]<-hash_string(rawdf$Product.Name[ctr])
}
###################################################################################
######## STEP 1. TAGGING EXACT MATCH DESCRIPTIONS MATCHING MPN+ITEM
###################################################################################
## Retrieve the unique hashes and the count of their occurence
Unique.Hashes<-unique(rawdf$Hash.Value)
numHash<-length(Unique.Hashes)
Ocurrence<-numeric(numHash)
for (ctr in 1:numHash){
Ocurrence[ctr]<-sum(rawdf$Hash.Value==Unique.Hashes[ctr])
}
## Create a Numeric array with Uniques + their Occurences
exactDupeTracker<-cbind(Unique.Hashes,Ocurrence)
## Get indexes of hashes that occur more than once from the above numeric array
dupeIdx<-which(exactDupeTracker[,2]>1)
## Quantify the actual number of exact matches
numExactDupes<-length(dupeIdx)
## From the numeric array exactDupeTracker, retrieve each hash value that
## occurs more than once. Then look up the original dataframe to get all the rows
## where a given dupeHash occurs.
## Tag all these rows with a common SKUFID number
for (ctr in 1:numExactDupes){
dupeHash<-exactDupeTracker[dupeIdx[ctr],1]
dupeHashIdx<-which(rawdf$Hash.Value==dupeHash)
rawdf[dupeHashIdx,23]<-SKUFID_CTR
SKUFID_CTR<-SKUFID_CTR + 1
}
clusterMaster$tagged<-0
for(ctr in 1:numClusters){
clusterMaster$tagged[ctr]<-
length(which(rawdf$CID==clusterMaster$clusterID[ctr]&&
rawdf$SKUFID!=0))
}
View(clusterMaster)
which(clusterMaster$size>0)
which(clusterMaster$tagged>0)
range(rawdf$CID)
choose(300000,2)
choose(300000,2)/10^9
choose(1000000,2)/10^9
library(stringr)
library(textreuse)
workingDirectory<-"C:\\Users\\koushikk\\repos\\XeevaIDMPrototypes\\cat"
# contains SKUs that have to be matched
file1<-"C:\\Users\\koushikk\\repos\\XeevaIDMPrototypes\\cat\\LearBrazil.csv"
setwd(workingDirectory)
# read both files into dataframes
rawdf1 = read.csv(file1,stringsAsFactors=FALSE)
matchResults<-match(rawdf1$PRODUCT_SKU,rawdf1$DECOMM_SKU,nomatch = -1)
idx<-which(matchResults>0)
matchResults[idx]<-1
write.csv(matchResults,"MatchResultsbrazil.csv")
library(stringr)
library(textreuse)
workingDirectory<-"C:\\Users\\koushikk\\repos\\XeevaIDMPrototypes\\cat"
# contains SKUs that have to be matched
file1<-"C:\\Users\\koushikk\\repos\\XeevaIDMPrototypes\\cat\\ROW.csv"
setwd(workingDirectory)
# read both files into dataframes
rawdf1 = read.csv(file1,stringsAsFactors=FALSE)
matchResults<-match(rawdf1$PRODUCT_SKU,rawdf1$DECOMM_SKU,nomatch = -1)
idx<-which(matchResults>0)
matchResults[idx]<-1
write.csv(matchResults,"MatchResultsrow.csv")
