## Define a function that computes tfidf weights from a term frequency vector
## and a document frequency scalar
getTfIdfWeights<-function(tfVector,df){
weight = rep(0, length(tfVector)) # initialize weights to zero
weight[tfVector > 0] = (1 + log2(tfVector[tfVector > 0])) * log2(numDocs/df)
weight
}
## Define a function returns the weights for every term vector
getWeightsperTermVector<-function(tfIdfRow){
termDf<-sum(tfIdfRow[1:numDocs]>0)
tdIdfVector<-getTfIdfWeights(tfIdfRow,termDf)
return(tdIdfVector)
}
## Obtain the Matrix that houses the weighted values for the term vectors
tfIdfMatrix<-t(apply(termDocMatrix,c(1),FUN=getWeightsperTermVector))
colnames(tfIdfMatrix)<-colnames(termDocMatrix)
## Remove non numeric values - general cleanup
tfIdfMatrix[which(!is.finite(tfIdfMatrix))]<-0
## norm each vector to one
tfIdfMatrix<-scale(tfIdfMatrix,center=FALSE,scale=
sqrt(colSums(tfIdfMatrix^2)))
rm(docList,documentCorpus,termDocMatrix)
########## Step #4: Test the model on test data  ################################
## Define a function that classifies  input itemText using a trained
## tfIdfMatrix
classifyItem<-function(itemText,trainedTfIdfMatrix){
## create the input Vector by following the same pre-processing
## used in training
queryList<-VectorSource(itemText)
testCorpus<-Corpus(queryList)
testCorpus<-tm_map(testCorpus,removePunctuation)
testCorpus<-tm_map(testCorpus,tolower)
testCorpus<-tm_map(testCorpus,stripWhitespace)
## Compute the termDocumentMatrix for the query
termDocMatrixTest<- as.matrix(TermDocumentMatrix(testCorpus,
control=list(dictionary=dictionaryOfTerms))
)
## To compute the weights for terms, initialize to zero, and calculate
## the weight for all non-zero occurence terms
queryTfIdf<-rep(0, numWords)
nDocs<-dim(trainedTfIdfMatrix)[2]
for (l in 1:numWords){
if(termDocMatrixTest[l]>0){
queryTfIdf[l]<-
(1 + log2(termDocMatrixTest[l])) *
log2(nDocs)
}
}
## clean-up any non numeric values
queryTfIdf[which(!is.finite(queryTfIdf))]<-0
## norm vector values to one as before
queryTfIdf<-scale(queryTfIdf,center=FALSE,scale=
sqrt(sum(queryTfIdf^2)))
## compute the angle beween query and training vectors
queryscores<-t(queryTfIdf) %*% trainedTfIdfMatrix
## rank in order of closeness
rd<-data.frame(doc=names(training),score=t(queryscores))
rd<-rd[order(rd$score,decreasing=TRUE),]
return(as.character(rd[1,1]))
}
## Define a test bed of data and store classification results
testListSize<-length(testing)
testBedvone<-data.frame()
for (z in 1:testListSize){
listOfItemInGroup<-testing[[z]]
numItemsInGroup<-length(listOfItemInGroup)
numItemsToSample<-numItemsInGroup
if(numItemsInGroup>testsPerGroup){
numItemsToSample<-testsPerGroup
}
idx<-sample(1:numItemsInGroup,numItemsToSample,replace=F)
itemCategory<-rep(names(testing[z]),numItemsToSample)
testBedRows<-data.frame(itemCategory,
listOfItemInGroup[idx],stringsAsFactors=FALSE)
testBedvone<-rbind(testBedvone,testBedRows)
}
numTestExamples<-nrow(testBedvone)
testBedvone$output<-c(" ")
colnames(testBedvone)<-c("group","itemtext","output")
### Individual sample testing
#testBedvone$itemtext[2]
#testBedvone$group[2]
#classifyItem("Trigger 600R-025  ",tfIdfMatrix)
### Testing a list
t2<-Sys.time()
for (ctr in 1:numTestExamples) {
testBedvone$output[ctr]<-classifyItem(
testBedvone$itemtext[ctr],tfIdfMatrix)
}
setdiff(testBedvone$group,testBedvone$output)
## check if there are groups in the prediction set not in the test set.
groupDiff<-setdiff(testBedvone$group,testBedvone$output)
numMissingGroups<-len(groupDiff)
numMissingGroups<-length(groupDiff)
if(numMissingGroups>0){
for (cleanCtr in 1:numMissingGroups){
testBedvone<-rbind(testBedvone,
c(groupDiff[cleanCtr],"dummy text",
c(groupDiff[cleanCtr])))
}
}
View(testBedvone)
testBedvone$group<-as.factor(testBedvone$group)
testBedvone$output<-as.factor(testBedvone$output)
accuracy<-confusionMatrix(testBedvone$output,testBedvone$group)$overall[1]
runResults<-paste(as.character(level),numRecords,
length(testingSet),minimumRowsForTraining,
accuracy,
as.double(difftime(t3,t1,units="mins")),sep=',')
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\PreProcessedInputP2R1.0.csv" # source file
minimumRowsForTraining<-40 # min rows in data need to consider for training
trainingSetFraction<-0.8 # amount of data set used for training Vs testing
numRecords<-100000 #number of lines to read
level<-1 # 1= Category,2 = Sub-Category
testsPerGroup<-5 # number of records per group that testing will be performed on
runBookName<-"XeevaClientDataRunBook.csv" # name of file to record results
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
cat("From start to training completion:",as.double(p4-p1))
cat("Just for testing:",as.double(p5-p4))
predictions[[2]]$overall
## write results into run book
runResults<-paste(as.character(level),numRecords,
length(testingSet),minimumRowsForTraining,
predictions[[2]]$overall[1],
as.double(difftime(p5,p1,units="mins")),sep=',')
write.table(runResults,runBookName,append=TRUE,col.names=FALSE,quote=FALSE)
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\PreProcessedInputP2R1.0.csv" # source file
minimumRowsForTraining<-20 # min rows in data need to consider for training
trainingSetFraction<-0.8 # amount of data set used for training Vs testing
numRecords<-10000 #number of lines to read
level<-1 # 1= Category,2 = Sub-Category
testsPerGroup<-10 # number of records per group that testing will be performed on
runBookName<-"XeevaClientDataRunBook.csv" # name of file to record results
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
cat("From start to training completion:",as.double(p4-p1))
cat("Just for testing:",as.double(p5-p4))
predictions[[2]]$overall
## write results into run book
runResults<-paste(as.character(level),numRecords,
length(testingSet),minimumRowsForTraining,
predictions[[2]]$overall[1],
as.double(difftime(p5,p1,units="mins")),sep=',')
write.table(runResults,runBookName,append=TRUE,col.names=FALSE,quote=FALSE)
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\PreProcessedInputP2R1.0.csv" # source file
minimumRowsForTraining<-20 # min rows in data need to consider for training
trainingSetFraction<-0.8 # amount of data set used for training Vs testing
numRecords<-50000 #number of lines to read
level<-1 # 1= Category,2 = Sub-Category
testsPerGroup<-10 # number of records per group that testing will be performed on
runBookName<-"XeevaClientDataRunBook.csv" # name of file to record results
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
cat("From start to training completion:",as.double(p4-p1))
cat("Just for testing:",as.double(p5-p4))
predictions[[2]]$overall
## write results into run book
runResults<-paste(as.character(level),numRecords,
length(testingSet),minimumRowsForTraining,
predictions[[2]]$overall[1],
as.double(difftime(p5,p1,units="mins")),sep=',')
write.table(runResults,runBookName,append=TRUE,col.names=FALSE,quote=FALSE)
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\PreProcessedInputP2R1.0.csv" # source file
minimumRowsForTraining<-20 # min rows in data need to consider for training
trainingSetFraction<-0.8 # amount of data set used for training Vs testing
numRecords<-100000 #number of lines to read
level<-1 # 1= Category,2 = Sub-Category
testsPerGroup<-10 # number of records per group that testing will be performed on
runBookName<-"XeevaClientDataRunBook.csv" # name of file to record results
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
cat("From start to training completion:",as.double(p4-p1))
cat("Just for testing:",as.double(p5-p4))
predictions[[2]]$overall
## write results into run book
runResults<-paste(as.character(level),numRecords,
length(testingSet),minimumRowsForTraining,
predictions[[2]]$overall[1],
as.double(difftime(p5,p1,units="mins")),sep=',')
write.table(runResults,runBookName,append=TRUE,col.names=FALSE,quote=FALSE)
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\PreProcessedInputP2R1.0.csv" # source file
minimumRowsForTraining<-20 # min rows in data need to consider for training
trainingSetFraction<-0.8 # amount of data set used for training Vs testing
numRecords<-10000 #number of lines to read
level<-2 # 1= Category,2 = Sub-Category
testsPerGroup<-10 # number of records per group that testing will be performed on
runBookName<-"XeevaClientDataRunBook.csv" # name of file to record results
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
cat("From start to training completion:",as.double(p4-p1))
cat("Just for testing:",as.double(p5-p4))
predictions[[2]]$overall
## write results into run book
runResults<-paste(as.character(level),numRecords,
length(testingSet),minimumRowsForTraining,
predictions[[2]]$overall[1],
as.double(difftime(p5,p1,units="mins")),sep=',')
write.table(runResults,runBookName,append=TRUE,col.names=FALSE,quote=FALSE)
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\PreProcessedInputP2R1.0.csv" # source file
minimumRowsForTraining<-20 # min rows in data need to consider for training
trainingSetFraction<-0.8 # amount of data set used for training Vs testing
numRecords<-50000 #number of lines to read
level<-2 # 1= Category,2 = Sub-Category
testsPerGroup<-10 # number of records per group that testing will be performed on
runBookName<-"XeevaClientDataRunBook.csv" # name of file to record results
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
cat("From start to training completion:",as.double(p4-p1))
cat("Just for testing:",as.double(p5-p4))
predictions[[2]]$overall
## write results into run book
runResults<-paste(as.character(level),numRecords,
length(testingSet),minimumRowsForTraining,
predictions[[2]]$overall[1],
as.double(difftime(p5,p1,units="mins")),sep=',')
write.table(runResults,runBookName,append=TRUE,col.names=FALSE,quote=FALSE)
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\PreProcessedInputP2R1.0.csv" # source file
minimumRowsForTraining<-20 # min rows in data need to consider for training
trainingSetFraction<-0.8 # amount of data set used for training Vs testing
numRecords<-100000 #number of lines to read
level<-2 # 1= Category,2 = Sub-Category
testsPerGroup<-10 # number of records per group that testing will be performed on
runBookName<-"XeevaClientDataRunBook.csv" # name of file to record results
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
cat("From start to training completion:",as.double(p4-p1))
cat("Just for testing:",as.double(p5-p4))
predictions[[2]]$overall
## write results into run book
runResults<-paste(as.character(level),numRecords,
length(testingSet),minimumRowsForTraining,
predictions[[2]]$overall[1],
as.double(difftime(p5,p1,units="mins")),sep=',')
write.table(runResults,runBookName,append=TRUE,col.names=FALSE,quote=FALSE)
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\PreProcessedInputP2R1.0.csv" # source file
minimumRowsForTraining<-20 # min rows in data need to consider for training
trainingSetFraction<-0.8 # amount of data set used for training Vs testing
numRecords<-150000 #number of lines to read
level<-1 # 1= Category,2 = Sub-Category
testsPerGroup<-10 # number of records per group that testing will be performed on
runBookName<-"XeevaClientDataRunBook.csv" # name of file to record results
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
cat("From start to training completion:",as.double(p4-p1))
cat("Just for testing:",as.double(p5-p4))
predictions[[2]]$overall
## write results into run book
runResults<-paste(as.character(level),numRecords,
length(testingSet),minimumRowsForTraining,
predictions[[2]]$overall[1],
as.double(difftime(p5,p1,units="mins")),sep=',')
write.table(runResults,runBookName,append=TRUE,col.names=FALSE,quote=FALSE)
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\PreProcessedInputP2R1.0.csv" # source file
minimumRowsForTraining<-20 # min rows in data need to consider for training
trainingSetFraction<-0.8 # amount of data set used for training Vs testing
numRecords<-150000 #number of lines to read
level<-2 # 1= Category,2 = Sub-Category
testsPerGroup<-10 # number of records per group that testing will be performed on
runBookName<-"XeevaClientDataRunBook.csv" # name of file to record results
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
cat("From start to training completion:",as.double(p4-p1))
cat("Just for testing:",as.double(p5-p4))
predictions[[2]]$overall
## write results into run book
runResults<-paste(as.character(level),numRecords,
length(testingSet),minimumRowsForTraining,
predictions[[2]]$overall[1],
as.double(difftime(p5,p1,units="mins")),sep=',')
write.table(runResults,runBookName,append=TRUE,col.names=FALSE,quote=FALSE)
fileLocation
<-"C:\\Users\\koushikk\\repos\\XeevaIDM\\IR1.2\\XeevaClientDataRunBook.csv"
runResults<-read.csv(fileLocation)
runResults
fileLocation<-"C:\\Users\\koushikk\\repos\\XeevaIDM\\IR1.2\\XeevaClientDataRunBook.csv"
runResults<-read.csv(fileLocation)
runResults
library(ggplot2)
qplot(Num.Records,Accuracy,data=runResults)
qplot(Num.Records,Accuracy,data=runResults,color=Level)
qplot(Num.Records,Accuracy,data=runResults,color=Level,geom="smooth")
qplot(Num.Records,Accuracy,data=runResults,color=Level,geom="smooth",method="loess")
?qplot
?qplot
?jcall
library(rjava)
