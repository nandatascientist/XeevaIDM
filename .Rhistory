## used in training
queryList<-VectorSource(itemText)
testCorpus<-Corpus(queryList)
testCorpus<-tm_map(testCorpus,removePunctuation)
testCorpus<-tm_map(testCorpus,tolower)
testCorpus<-tm_map(testCorpus,stripWhitespace)
## Compute the termDocumentMatrix for the query
termDocMatrixTest<- as.matrix(TermDocumentMatrix(testCorpus,
control=list(dictionary=dictionaryOfTerms))
)
## To compute the weights for terms, initialize to zero, and calculate
## the weight for all non-zero occurence terms
queryTfIdf<-rep(0, numWords)
nDocs<-dim(trainedTfIdfMatrix)[2]
for (l in 1:numWords){
if(termDocMatrixTest[l]>0){
queryTfIdf[l]<-
(1 + log2(termDocMatrixTest[l])) *
log2(nDocs)
}
}
## clean-up any non numeric values
queryTfIdf[which(!is.finite(queryTfIdf))]<-0
## norm vector values to one as before
queryTfIdf<-scale(queryTfIdf,center=FALSE,scale=
sqrt(sum(queryTfIdf^2)))
## compute the angle beween query and training vectors
queryscores<-t(queryTfIdf) %*% trainedTfIdfMatrix
## rank in order of closeness
rd<-data.frame(doc=names(training),score=t(queryscores))
rd<-rd[order(rd$score,decreasing=TRUE),]
return(as.character(rd[1,1]))
}
## Define a test bed of data and store classification results
t1<-Sys.time()
testListSize<-length(testing)
testBedvone<-data.frame()
for (z in 1:testListSize){
listItem<-testing[[z]]
listItemLength<-length(listItem)
itemCategory<-rep(names(testing[z]),listItemLength)
testBedRows<-data.frame(itemCategory,listItem,stringsAsFactors=FALSE)
testBedvone<-rbind(testBedvone,testBedRows)
}
numTestExamples<-nrow(testBedvone)
testBedvone$output<-c(" ")
#testBedvone$listItem[2]
#testBedvone$itemCategory[2]
#classifyItem(testBedvone$listItem[2],tfIdfMatrix)
t2<-Sys.time()
for (ctr in 1:numTestExamples) {
testBedvone$output[ctr]<-classifyItem(testBedvone$listItem[ctr],tfIdfMatrix)
}
t3<-Sys.time()
t2-t1
t3-t2
983/120
2.1*60
122*60
7320/4800
a<-c("whatra","rawhat","preeta","tapreet","machaku")
idx<-c(3,4,5)
a[idx]
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\amazon_supply_Demo_Output_03-07-15.csv" # name of file in current dir
minimumRowsForTraining<-10 # min representation in data for consideration in training
trainingSetFraction<-0.9 # amount of data set used for training Vs testing
numRecords<-10000 #number of lines to read
level<-1 # 1= Segment,2 = Family, 3 = Class, 4 = Commodity
testsPerGroup<-10
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
pwd
wd
setwd("C:/Users/koushikk/repos/XeevaIDM")
source("classifyItem.R")
predictOutput<-function(testDataSet,trainedModel,sampleSizePerGroup){
# get the number of groups which is same as testing data set length
testListSize<-length(testDataSet)
testBed<-data.frame()
for (z in 1:testListSize){
# extract the query text array for each group
listOfItemInGroup<-testDataSet[[z]]
# get the number of items in current group
numItemsInGroup<-length(listOfItemInGroup)
# default number of items to test per group to existing size
numItemsToSample<-numItemsInGroup
# if existing size is greater than suggested sample size, override
if(numItemsInGroup>sampleSizePerGroup){
numItemsToSample<-sampleSizePerGroup
}
# get the ramdomized indexes of items to use in testing
idx<-sample(1:numItemsInGroup,numItemsToSample,replace=F)
# fill in group names as many times as there are test examples
itemCategory<-rep(names(testingSet[z]),numItemsToSample)
#create a data frame with group name + ramdomly picked items
testBedRows<-data.frame(itemCategory,listOfItemInGroup[idx],stringsAsFactors=FALSE)
#add to testbed
testBed<-rbind(testBed,testBedRows)
}
# compute the number of examples in test bed
numTestExamples<-nrow(testBed)
# create an empty column for classification output
testBed$output<-c(" ")
for (ctr in 1:numTestExamples) {
# for each item in the test bed, perform classification using
# trained model
testBed$output[ctr]<-classifyItem(testBed$listItem[ctr],
trainedModel)
}
# return updated test bed with associated confusion matrix
return(list(testBed,
confusionMatrix(testBed$output,testBed$itemCategory)))
}
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\amazon_supply_Demo_Output_03-07-15.csv" # name of file in current dir
minimumRowsForTraining<-10 # min representation in data for consideration in training
trainingSetFraction<-0.9 # amount of data set used for training Vs testing
numRecords<-10000 #number of lines to read
level<-1 # 1= Segment,2 = Family, 3 = Class, 4 = Commodity
testsPerGroup<-10
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\amazon_supply_Demo_Output_03-07-15.csv" # name of file in current dir
minimumRowsForTraining<-10 # min representation in data for consideration in training
trainingSetFraction<-0.9 # amount of data set used for training Vs testing
numRecords<-10000 #number of lines to read
level<-1 # 1= Segment,2 = Family, 3 = Class, 4 = Commodity
testsPerGroup<-10
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
p5-p4
p5-p1
predictions[[2]]$overall[1]
predictions[[2]]$overall
predictions[[2]]$overall
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\amazon_supply_Demo_Output_03-07-15.csv" # source file
minimumRowsForTraining<-10 # min rows in data need to consider for training
trainingSetFraction<-0.9 # amount of data set used for training Vs testing
numRecords<-10000 #number of lines to read
level<-1 # 1= Segment,2 = Family, 3 = Class, 4 = Commodity
testsPerGroup<-10 # number of records per group that testing will be performed on
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
p5-p1
predictions[[2]]$overall
?print
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\amazon_supply_Demo_Output_03-07-15.csv" # source file
minimumRowsForTraining<-10 # min rows in data need to consider for training
trainingSetFraction<-0.9 # amount of data set used for training Vs testing
numRecords<-10000 #number of lines to read
level<-1 # 1= Segment,2 = Family, 3 = Class, 4 = Commodity
testsPerGroup<-10 # number of records per group that testing will be performed on
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
cat("From start to training completion:",as.double(p4-p1))
cat("Just for testing:",as.double(p5-p4))
predictions[[2]]$overall
rm()
rm(WS, list = WS)
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
rm(WS, list = WS)
c(ls())
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\amazon_supply_Demo_Output_03-07-15.csv" # source file
minimumRowsForTraining<-10 # min rows in data need to consider for training
trainingSetFraction<-0.9 # amount of data set used for training Vs testing
numRecords<-10000 #number of lines to read
level<-1 # 1= Segment,2 = Family, 3 = Class, 4 = Commodity
testsPerGroup<-10 # number of records per group that testing will be performed on
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
cat("From start to training completion:",as.double(p4-p1))
cat("Just for testing:",as.double(p5-p4))
predictions[[2]]$overall
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\amazon_supply_Demo_Output_03-07-15.csv" # source file
minimumRowsForTraining<-10 # min rows in data need to consider for training
trainingSetFraction<-0.9 # amount of data set used for training Vs testing
numRecords<-10000 #number of lines to read
level<-2 # 1= Segment,2 = Family, 3 = Class, 4 = Commodity
testsPerGroup<-10 # number of records per group that testing will be performed on
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
cat("From start to training completion:",as.double(p4-p1))
cat("Just for testing:",as.double(p5-p4))
predictions[[2]]$overall
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\amazon_supply_Demo_Output_03-07-15.csv" # source file
minimumRowsForTraining<-10 # min rows in data need to consider for training
trainingSetFraction<-0.9 # amount of data set used for training Vs testing
numRecords<-10000 #number of lines to read
level<-3 # 1= Segment,2 = Family, 3 = Class, 4 = Commodity
testsPerGroup<-10 # number of records per group that testing will be performed on
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
cat("From start to training completion:",as.double(p4-p1))
cat("Just for testing:",as.double(p5-p4))
predictions[[2]]$overall
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\amazon_supply_Demo_Output_03-07-15.csv" # source file
minimumRowsForTraining<-10 # min rows in data need to consider for training
trainingSetFraction<-0.9 # amount of data set used for training Vs testing
numRecords<-10000 #number of lines to read
level<-3 # 1= Segment,2 = Family, 3 = Class, 4 = Commodity
testsPerGroup<-10 # number of records per group that testing will be performed on
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
cat("From start to training completion:",as.double(p4-p1))
cat("Just for testing:",as.double(p5-p4))
predictions[[2]]$overall
sample(1:1,1,replace=F)
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\amazon_supply_Demo_Output_03-07-15.csv" # source file
minimumRowsForTraining<-10 # min rows in data need to consider for training
trainingSetFraction<-0.9 # amount of data set used for training Vs testing
numRecords<-10000 #number of lines to read
level<-3 # 1= Segment,2 = Family, 3 = Class, 4 = Commodity
testsPerGroup<-10 # number of records per group that testing will be performed on
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Train a VSM model based on training data
p3<-Sys.time()
trainedVSMModel<-trainVSMonData(trainingSet)
## Predict classification on test data using trained model
p4<-Sys.time()
predictions<-predictOutput(testingSet,trainedVSMModel,testsPerGroup)
p5<-Sys.time()
cat("From start to training completion:",as.double(p4-p1))
cat("Just for testing:",as.double(p5-p4))
predictions[[2]]$overall
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\amazon_supply_Demo_Output_03-07-15.csv" # source file
minimumRowsForTraining<-10 # min rows in data need to consider for training
trainingSetFraction<-0.9 # amount of data set used for training Vs testing
numRecords<-10000 #number of lines to read
level<-3 # 1= Segment,2 = Family, 3 = Class, 4 = Commodity
testsPerGroup<-10 # number of records per group that testing will be performed on
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
## Load the needed libraries and functions
library(tm)
library(SnowballC)
library(caret)
source("preProcessFileData.R")
source("splitDataSet.R")
source("trainVSMonData.R")
source("classifyItem.R")
source("predictOutput.R")
## Set script parameters
set.seed(12345) # for repeatability of random numbers
rawDatafileName<-"c:\\idm\\amazon_supply_Demo_Output_03-07-15.csv" # source file
minimumRowsForTraining<-10 # min rows in data need to consider for training
trainingSetFraction<-0.9 # amount of data set used for training Vs testing
numRecords<-10000 #number of lines to read
level<-3 # 1= Segment,2 = Family, 3 = Class, 4 = Commodity
testsPerGroup<-10 # number of records per group that testing will be performed on
## get usable data from file
p1<-Sys.time()
processedData<-preprocessFileData(rawDatafileName,level,
minimumRowsForTraining,numRecords)
## segment data into testing and training sets
p2<-Sys.time()
segList<-splitDataSet(processedData,trainingSetFraction)
trainingSet<-segList[[1]]
testingSet<-segList[[2]]
